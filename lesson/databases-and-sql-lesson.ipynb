{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![General Assembly Logo](https://camo.githubusercontent.com/1a91b05b8f4d44b5bbfb83abac2b0996d8e26c92/687474703a2f2f692e696d6775722e636f6d2f6b6538555354712e706e67)](https://generalassemb.ly/education/web-development-immersive)\n",
    "![Misk Logo](https://i.ibb.co/KmXhJbm/Webp-net-resizeimage-1.png)\n",
    "\n",
    "*Instructor: Marcus Lim*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databases and SQL\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "### Core\n",
    "\n",
    "* Connect to a local or remote SQLite database with the command line, Python, and `pandas`\n",
    "* Perform simple table-level read queries, including:\n",
    "    * Taking subsets of columns with `SELECT`\n",
    "    * Taking subsets of rows with `WHERE`, `AND`, and `OR`\n",
    "    * Aggregating data with `GROUP BY` and `HAVING`\n",
    "    * Joining tables with `JOIN`\n",
    "* Eliminate duplicates with `SELECT DISTINCT`\n",
    "* Perform queries requiring subqueries\n",
    "* Compare strings with `LIKE`\n",
    "* Process the output of queries with `AS`, `LIMIT`, and `ORDER BY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Thus far, you have been performing operations on datasets that can fit on local storage (your computer's hard disk or SSD). However, in a production context, it is far more common for companies to store data on some remote server and extract subsets of it for analysis where necessary. This is for two reasons.\n",
    "\n",
    "Firstly, the amount of data that a company has might be measurable only in terabytes (thousands of gigabytes). That won't fit on your computer. Also, while `pandas` works great for interactive analysis, it's not built for stability and concurrency. \n",
    "\n",
    "This means that data is often stored not in CSVs and JSONs, but rather in *databases*. When you work with a database, you don't need to worry about *how* it stores data, but only about how to access it. At this point, we'll be focusing on *relational databases*, which allow you to treat data like tables of rows and columns, just like you're used to in `pandas`. The strength of these databases is that they make four strong guarantees, which you can remember with **ACID**:\n",
    "\n",
    "* **A**tomicity (what happens when reads and writes occur simultaneously?)\n",
    "* **C**onsistency (how do we prevent data corruption?)\n",
    "* **I**solation (how do we support multiple connections at once?)\n",
    "* **D**urability (what happens to the data if we lose power?)\n",
    "\n",
    "The most common way to interact with such databases, by far, is some variant of *Structured Query Language* (SQL). While there exist multiple dialects (sqlite, MySQL, PostgreSQL, Oracle...), the differences are largely minor and syntactical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with databases\n",
    "\n",
    "We will go through three methods to access and query a database:\n",
    "\n",
    "* Python using `pandas`\n",
    "* Python using `sqlite3`\n",
    "* The command line\n",
    "\n",
    "Note for Windows users: you may need to download and install the sqlite3 utilities [here](https://sqlite.org/2020/sqlite-tools-win32-x86-3310100.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL with `pandas`\n",
    "\n",
    "`pandas` can interact with an SQL database using the top-level `read_sql` function. Internally, it uses an `sqlite3` connection object to do so; however, since we are more familiar with `pandas`, we will first look at how we can use `pandas` functionality to perform simple queries, before going specifically into `sqlite3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection object\n",
    "\n",
    "conn = sqlite3.connect('titanic_sqlite3')\n",
    "df = pd.read_sql('SELECT * FROM titanic', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking column subsets\n",
    "\n",
    "The `SELECT` class is the first piece of SQL we will learn. It takes the following format:\n",
    "\n",
    "```sql\n",
    "SELECT <column_names> \n",
    "FROM <table_name>\n",
    "```\n",
    "\n",
    "The Titanic data has been stored in the `titanic` table. Therefore, for example, if we wanted to get the `survived` and `pclass` columns from it, we would execute `SELECT survived, pclass FROM titanic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT survived, pclass\n",
    "FROM titanic\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want all the columns? In this case, SQL provides a special wildcard, `*`. It's okay to use a bare `SELECT *` in this case because we have very few rows, but note that doing this on a database with, say, a hundred million rows will probably just lead to a timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT * \n",
    "FROM titanic'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say in general that a `SELECT` class is equivalent to `df[column_names]`, where `column_names` is a `list` of column names in `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique values\n",
    "\n",
    "We can modify the `SELECT` clause to `SELECT DISTINCT` to get only the unique values in a column, or the unique combinations of values in a number of columns. Above, we got the values in the `survived` and `pclass` columns for all passengers. With `SELECT DISTINCT`, we can see what pairs exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT DISTINCT survived, pclass\n",
    "FROM titanic\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking row subsets\n",
    "\n",
    "Sometimes, as in `pandas`, we want to filter rows based on the values of certain columns, which is done with a `WHERE` clause, which has a format as follows: \n",
    "\n",
    "```sql\n",
    "SELECT <column_names> \n",
    "FROM <table_name> \n",
    "WHERE <condition>\n",
    "```\n",
    "\n",
    "The condition can be expressed as it is in normal Python, with operators such as `!=` and `<`. Note, however, that the equality operator is `=`, not `==`.\n",
    "\n",
    "Say we want all columns where `survived` is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "WHERE survived = 1\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result['survived'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in `pandas`, we may also want to combine conditions, which we can do with `AND` and `OR`. We can add on an additional filter, selecting only those rows where `age` is less than 20 or more than 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "WHERE survived = 1\n",
    "AND (age < 20 OR age > 50)\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result['age'].between(20, 50).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how you would perform these queries in `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String comparisons\n",
    "\n",
    "Apart from the standard operators you know, SQL has a `LIKE` clause, which allows you to specify a sort of pattern for strings, with two wildcards:\n",
    "\n",
    "* `%`, which represents any number of (possibly different) characters\n",
    "* `_`, which represents any single character\n",
    "\n",
    "So, for example, to get all the rows where the `name` column contains `Mr.`:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM titanic\n",
    "WHERE name LIKE '%Mr.%'\n",
    "```\n",
    "\n",
    "The reverse operation (match those strings which do *not* conform to this pattern) can be performed with `NOT LIKE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "WHERE name LIKE '%Mr.%'\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise\n",
    "\n",
    "Let's practice some SQL! For the questions below, answer them first using solely `pandas`, and then write a SQL query and execute it with `pd.read_sql`. Check your answers against the `pandas` result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select the `name`, `sex` and `age` columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Get all the rows where `fare` is less than 50.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Get the unique combinations of the `survived`, `pclass` and `embarked` columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Get the ages of all the male passengers whose Passenger ID is more than 400.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Get the names of the female passengers whose names do not contain \"Miss.\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "The result of a query can be sorted with the `ORDER BY` clause, which has the following format:\n",
    "\n",
    "```sql\n",
    "SELECT ...\n",
    "ORDER BY <order_column_names>\n",
    "```\n",
    "\n",
    "You may optionally specify ascending order with `ASC` (the default) and descending order with `DESC` on a column-by-column basis. So, for example, if you wanted to order by the `sibsp` column in ascending order and then the `parch` column in descending order, you could execute the following query:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM titanic\n",
    "ORDER BY sibsp ASC,\n",
    "         parch DESC\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "ORDER BY sibsp ASC,\n",
    "         parch DESC\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting results\n",
    "\n",
    "In some cases, you only want a subset of the returned results; for example, the first three. This is achieved with the `LIMIT` clause, which has this format:\n",
    "\n",
    "```sql\n",
    "SELECT ...\n",
    "LIMIT <result_count>\n",
    "```\n",
    "\n",
    "If the number of results returned is lower than the specified result count, then the `LIMIT` clause has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "LIMIT 7\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating\n",
    "\n",
    "You may also wish to perform aggregation operations, which can be done with function-like clauses. The ones you should know:\n",
    "\n",
    "* `COUNT`\n",
    "* `AVG`\n",
    "* `SUM`\n",
    "* `MIN`\n",
    "* `MAX`\n",
    "\n",
    "They operate the same way as in `pandas` - `AVG` is the equivalent of `mean`. Note that `count` counts only *non-null* values!\n",
    "\n",
    "To use them, just surround column names with the desired aggregation function. For example, say we wanted, for all passengers, the following data:\n",
    "\n",
    "* The number of non-null values in the `cabin` column\n",
    "* The percentage of people who survived\n",
    "* The total fare paid\n",
    "* The maximum age\n",
    "* The minimum age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT COUNT(cabin), \n",
    "       AVG(survived), \n",
    "       SUM(fare), \n",
    "       MAX(age), \n",
    "       MIN(age)\n",
    "FROM titanic\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks almost right, except for the fact that the survival percentage is on the wrong scale (it needs to be multiplied by 100 to be a percentage). It turns out that you can also perform arithmetic operations in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT COUNT(cabin), \n",
    "       AVG(survived) * 100, \n",
    "       SUM(fare), \n",
    "       MAX(age), \n",
    "       MIN(age)\n",
    "FROM titanic\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it would help if we could rename the results to be more descriptive. This can be done with the `AS` clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT COUNT(cabin) AS cabin_count, \n",
    "       AVG(survived) * 100 AS survival_percentage, \n",
    "       SUM(fare) AS total_fare, \n",
    "       MAX(age) AS max_age, \n",
    "       MIN(age) AS min_age\n",
    "FROM titanic\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby aggregations\n",
    "\n",
    "This aggregation functionality is incomplete without the power of groupby operations, and in fact the `pandas` groupby functionality is inspired by SQL. The relevant clause is, of course, `GROUP BY`, which takes the following format:\n",
    "\n",
    "```sql\n",
    "SELECT <column_names> \n",
    "FROM <table_name> \n",
    "GROUP BY <groupby_column_names>\n",
    "```\n",
    "\n",
    "In `pandas`, if you wanted to get the max age by `pclass`, you would execute the following code:\n",
    "\n",
    "```python\n",
    "df.groupby('pclass')['age'].max()\n",
    "```\n",
    "\n",
    "The SQL equivalent:\n",
    "\n",
    "```sql\n",
    "SELECT MAX(age) \n",
    "FROM titanic \n",
    "GROUP BY pclass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT MAX(age) \n",
    "FROM titanic \n",
    "GROUP BY pclass\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike in `pandas`, the grouping column is not automatically included for you; if you want to know which value corresponds to which aggregation, you need to include it explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT pclass, MAX(age) \n",
    "FROM titanic \n",
    "GROUP BY pclass\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping on multiple columns is similarly supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT survived, pclass, MAX(age) \n",
    "FROM titanic \n",
    "GROUP BY survived, pclass\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is not necessary, you can combine a `WHERE` clause with a `GROUP BY` clause so that you groupby only after filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT survived, pclass, MAX(age) \n",
    "FROM titanic \n",
    "WHERE sex = 'male'\n",
    "GROUP BY survived, pclass\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to the following:\n",
    "\n",
    "```python\n",
    "df[df['sex'] == 'male'].groupby(['survived', 'pclass']).max()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise 2\n",
    "\n",
    "More SQL practice. You know how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Get the highest value of the `sibsp` column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Get the ages of the 5 oldest passengers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Get the counts of passengers at each port of embarkation (that's the `embarked` column).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Get the percentage of passengers who survived, divided by passenger class, and then sex. Rename the columns in the result appropriately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Get the number of passengers in C-type cabins.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also impose conditions on the groups that are created by a `GROUP BY` clause. This is done not with `WHERE`, which is for filtering *before* the grouping process, but with `HAVING`:\n",
    "\n",
    "```sql\n",
    "SELECT <column_names>\n",
    "FROM <table_name>\n",
    "GROUP BY <groupby_column_name>\n",
    "```\n",
    "\n",
    "We can use this to, for example, get the values of `embarked` where the mean age is more than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT embarked\n",
    "FROM titanic\n",
    "GROUP BY embarked\n",
    "HAVING AVG(age) > 30\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to `df.groupby('embarked').filter(lambda g: g['age'].mean() > 30)['embarked'].unique()`, except that SQL also includes the null group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "Joins can be performed in SQL with the `JOIN` and `ON` clauses, in the following format:\n",
    "\n",
    "```sql\n",
    "SELECT <column_names>\n",
    "FROM <left_table_name>\n",
    "<JOIN_TYPE> JOIN <right_table_name>\n",
    "ON <left_table_name>.<left_table_join_column> = <right_table_name>.<right_table_join_column>\n",
    "```\n",
    "\n",
    "So, for example, say you want to join the `titanic` table (left) and the `classes` table (right):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM titanic\n",
    "LEFT JOIN classes\n",
    "ON titanic.pclass = classes.pclass\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has two `pclass` columns, one from each table. Perform the same query, but this time keeping only the `passengerid` and `pclass_name` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT passengerid, pclass_name\n",
    "FROM titanic\n",
    "LEFT JOIN classes\n",
    "ON titanic.pclass = classes.pclass\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have `None` as a value for `pclass_name`. To understand why, examine the `classes` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *\n",
    "FROM classes\n",
    "'''\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment on your own with other join types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you need to perform queries within queries. With what we have learnt above, it is impossible to get, for example, the IDs of all passengers who are older than average. In `pandas`, we would do it in this way:\n",
    "\n",
    "```python\n",
    "df.loc[df['age'] > df['age'].mean(), 'passengerid']\n",
    "```\n",
    "\n",
    "Notice that we need to compare the `age` column to another value that is itself derived from the `age` column. In such a case, we need a *subquery*:\n",
    "\n",
    "```sql\n",
    "SELECT passengerid\n",
    "FROM titanic\n",
    "WHERE age > (\n",
    "    SELECT AVG(age)\n",
    "    FROM titanic\n",
    ")\n",
    "AND age IS NOT NULL\n",
    "```\n",
    "\n",
    "In this subquery, we first get the average age from a query. We then *nest*, or insert, the result of that query into the larger query that makes a comparison on the `age` column, while selecting only the `passengerid` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT passengerid\n",
    "FROM titanic\n",
    "WHERE age > (\n",
    "    SELECT AVG(age)\n",
    "    FROM titanic\n",
    ")\n",
    "AND age IS NOT NULL\n",
    "'''\n",
    "\n",
    "result = pd.read_sql(sql, conn)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL with `sqlite3`\n",
    "\n",
    "The `sqlite3` library allows you to interact with a SQLite database at a lower level. `pandas` takes an SQL query and a connection object, hiding some parts of the hard work from us. Now, let's try to use `sqlite` directly. First, we need a *cursor object*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute SQL queries by calling `.execute()` on the cursor, which represents the set of results that are returned from a query. Notice that it is the `fetchall` call that actually interfaces with the database to retrieve data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "cursor.execute('SELECT * FROM titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "cursor.execute('SELECT * FROM titanic')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries we have learnt above deal with *reading* data. However, there may be cases in which you need to perform *insertions* or *modifications*, which we will talk about in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL with the command line\n",
    "\n",
    "You can access the database in this repo with `sqlite3 titanic_sqlite3`. All the above operations can be performed through the command line.\n",
    "\n",
    "Each dialect of SQL has its own client for accessing a database. For example, PostgreSQL uses `psql`. Consult the documentation of your client."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
